import os
import subprocess
import datetime, requests
from rich.console import Console
from docx import Document
from urllib.parse import urlparse, parse_qs
from pathlib import Path
import hashlib
import re
from dotenv import load_dotenv

from youtube_transcript_api import YouTubeTranscriptApi
from llama_index.core.schema import Document as LlamaDocument

from llama_index.core import StorageContext, load_index_from_storage, Settings
from llama_index.core.readers import SimpleDirectoryReader
from llama_index.core.indices.vector_store import VectorStoreIndex
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core.settings import Settings
from llama_index.readers.web import SimpleWebPageReader


from openai import OpenAI
# from llama_index.llms.openai import OpenAI as LlamaindexOpenAI


from llama_index.embeddings.huggingface import HuggingFaceEmbedding
# from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding

from .gwdg_llm import GWDGChatLLM


load_dotenv()


# === CONFIG ===
DRIVE_FOLDER_ID = "1vT4UTYHeFxS5Vy2u_OfQyQ6cQ-cP5Ywd"
API_KEY = os.getenv("GWDG_API_KEY") 
API_BASE = os.getenv("GWDG_API_BASE")

AZURE_VERSION = os.getenv("AZURE_API_VERSION")
AZURE_KEY = os.getenv("AZURE_CHAT_KEY")
AZURE_BASE = os.getenv("AZURE_API_BASE")

# base_dir = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = "./data" #os.path.join(base_dir, "/data")

# print('Base dir: ', base_dir, 'Data dir: ', DATA_DIR)
LOG_DIR = "./chat_logs" #os.path.join(base_dir, "/chat_logs")
STORAGE_DIR = "./lahn_index"#os.path.join(base_dir, "/lahn_index")


def download_drive_folder(folder_id, output_dir="./data"):
    print('Running download_drive_folder function...')
    os.makedirs(output_dir, exist_ok=True)
    cmd = f"gdown --folder https://drive.google.com/drive/folders/{folder_id} -O {output_dir}"
    subprocess.run(cmd, shell=True)


def fetch_system_prompt_from_gdoc(save=True):
    print(' Updating system prompt...')
    url = "https://docs.google.com/document/d/1NYOOy8KkaLDBwvHvEVg1hVDY5yvHeLACUpCEkJVM8Kw/export?format=txt"
    response = requests.get(url)
    response.raise_for_status()
    prompt = response.text.strip()
    # prompt = prompt[:prompt.find('General Internal Impressions')]

    if save==True:
        base_dir = os.path.dirname(os.path.abspath(__file__))
        file_path = os.path.join(base_dir, 'system_prompt.txt')
        with open(file_path, 'w') as f:
            f.write(prompt)
        print(' Done.')
    else:
        return prompt


def convert_docx_to_txt_and_cleanup(folder_path):
    for root, _, files in os.walk(folder_path):
        for file in files:
            file_path = os.path.join(root, file)
            if file.endswith('.docx') or '.' not in file:
                try:
                    doc = Document(file_path)
                    text = "\n".join([para.text for para in doc.paragraphs])
                    txt_filename = os.path.splitext(file)[0] + '.txt'
                    txt_path = os.path.join(root, txt_filename)
                    with open(txt_path, 'w', encoding='utf-8') as f:
                        f.write(text)
                    os.remove(file_path)
                    print(f"✅ Converted and deleted: {file_path}")
                except Exception as e:
                    print(f"❌ Failed to convert {file_path}: {e}")


def fetch_youtube_transcript(url, languages=["de"]):
    print(f"🔗 Fetching: {url}")
    try:
        parsed = urlparse(url)

        # Handle standard and short YouTube URL formats
        if "youtube.com" in parsed.netloc:
            video_id = parse_qs(parsed.query).get("v", [None])[0]
        elif "youtu.be" in parsed.netloc:
            video_id = parsed.path.lstrip("/")
        else:
            raise ValueError("Unrecognized YouTube URL format.")

        if not video_id or len(video_id) != 11:
            raise ValueError("Invalid YouTube video ID.")

        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=languages)
        full_text = " ".join([entry["text"] for entry in transcript])
        return LlamaDocument(text=full_text, metadata={"source": url})

    except Exception as e:
        print(f"❌ Failed to fetch {url}: {e}")
        return None


def sanitize_filename(url):
    domain = urlparse(url).netloc
    hashed = hashlib.md5(url.encode()).hexdigest()[:8]
    return f"{domain.replace('.', '_')}_{hashed}.txt"


def select_model():
    print("Choose a model:")
    print("1. Mistral")
    print("2. SauerKrautLM (Llama finetuned on German data)")
    choice = input("Enter 1 or 2: ")
    return "llama-3.1-sauerkrautlm-70b-instruct" if choice == "2" else "mistral-large-instruct"


def get_llm(mode='openai',model_name=None, system_prompt=None):
    base_dir = os.path.dirname(os.path.abspath(__file__))
    file_path = os.path.join(base_dir, 'system_prompt.txt')
    if system_prompt == None:
        system_prompt = open(file_path, 'r').read()

    # system_prompt += '\n You MUST ALWAYS call a function to answer any question. DO NOT respond directly. You have no knowledge or memory outside what you retrieve using the provided tools.\n'

    # if model_name != None:
    if mode == 'openai':

        llm =  OpenAI(
            # model=model_name,        # your HRZ model name
            # temperature=0.5,
            # system_prompt=system_prompt,
            # context_window=128000,

            # point at your custom endpoint:
            api_key=API_KEY,             # e.g. 'sk-…'
            base_url=API_BASE,           # "https://llm.hrz.uni-giessen.de/api/"
            # api_type="open_ai",          # use the “open_ai” protocol
            # api_version=None,            # leave None unless your server needs a version
        )


    else:

        llm = GWDGChatLLM(
                model=model_name,
                api_base=API_BASE,
                api_key=API_KEY,
                temperature=0.5,
                system_prompt=system_prompt
            )

    # print('LLM details: ', llm.model_dump())

    # Settings.llm = llm

    return llm, system_prompt


def create_session_log():
    os.makedirs(LOG_DIR, exist_ok=True)
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    return open(os.path.join(LOG_DIR, f"session_{timestamp}.txt"), "w")

def build_index():
    cmd = f"rm -r data/*"
    subprocess.run(cmd, shell=True)
    print('Just cleared data/ . Contents: ', os.listdir('data'))

    system_prompt = fetch_system_prompt_from_gdoc(save=False)
    system_prompt = system_prompt[:system_prompt.find('You also have access to sensory data for the river')]
    with open('data/system_prompt.txt', 'w') as f:
        f.write(system_prompt)
        print(' Added updated system prompt to embeddings.')


    print('Refreshing from Google Drive...')
    download_drive_folder(DRIVE_FOLDER_ID, DATA_DIR)
    convert_docx_to_txt_and_cleanup(DATA_DIR)

    print('Creating Vector store from data sources...')

    if len(os.listdir(DATA_DIR))>0:
        documents = SimpleDirectoryReader(DATA_DIR, recursive=True).load_data()
        print(f"{len(documents)} documents loaded from {DATA_DIR}")
        for i, doc in enumerate(documents):
            print(f"\n--- Document {i+1} ---")
            print("File:", doc.metadata.get('file_path', 'Unknown'))
            print("Content preview:", doc.text, "...\n")


    links_path = Path(DATA_DIR) / "General_News/Online News (Links).txt"
    if links_path.exists():
        with open(links_path, "r") as f:
            urls = [line.strip() for line in f if line.strip()]

        web_reader = SimpleWebPageReader()
        for url in urls:
            try:
                print(f"🔗 Fetching: {url}")
                if "youtube.com" in url or "youtu.be" in url:
                    doc = fetch_youtube_transcript(url)
                else:
                    docs = web_reader.load_data([url])
                    full_text = "\n\n".join(doc.text for doc in docs)
                    doc = LlamaDocument(text=full_text, metadata={"source": url})

                if doc:
                    filename = sanitize_filename(url)
                    filepath = Path(DATA_DIR) / "General_News/scraped_texts" / filename
                    filepath.parent.mkdir(parents=True, exist_ok=True)
                    with open(filepath, "w", encoding="utf-8") as f:
                        f.write(doc.text)
                    print(f"✅ Saved to {filepath}")
            except Exception as e:
                print(f"❌ Failed to fetch {url}: {e}")

    scraped_documents_path = Path(DATA_DIR) / "General_News/scraped_texts"
    if  scraped_documents_path.exists() and len(os.listdir(str(scraped_documents_path)))>0:
        scraped_documents = SimpleDirectoryReader(str(Path(DATA_DIR) / "General_News/scraped_texts")).load_data()
        documents += scraped_documents

    experiences_folder_path = Path(DATA_DIR) / "uploaded_experiences/text"
    # experiences_folder_is_empty = not os.listdir(str(Path(DATA_DIR) / "uploaded_experiences/text"))
    if experiences_folder_path.exists() and len(os.listdir(str(experiences_folder_path)))>0:
        user_experiences = SimpleDirectoryReader(str(Path(DATA_DIR) / "uploaded_experiences/text")).load_data()
        documents += user_experiences

    index = VectorStoreIndex.from_documents(documents)
    index.storage_context.persist(persist_dir=STORAGE_DIR)

    all_nodes = list(index.docstore.docs.values())
    for i, node in enumerate(all_nodes):
        print(f"\nNode {i}:")
        print(node.text)

    print('Done')

    return index



def build_or_load_index(refresh=False):
    # Settings.embed_model = AzureOpenAIEmbedding(
    #     model="text-embedding-3-large",
    #     deployment_name="text-embedding-3-large",
    #     api_key=AZURE_KEY,
    #     azure_endpoint=AZURE_BASE,
    #     api_version=AZURE_VERSION,
    # )

    Settings.embed_model = HuggingFaceEmbedding(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

    # GWDGEmbedding(
    #     api_key=API_KEY,
    #     api_base=API_BASE,
    #     model="e5-mistral-7b-instruct"
    # )

    index_ready = (
        os.path.exists(STORAGE_DIR)
        and os.path.exists(os.path.join(STORAGE_DIR, "docstore.json"))
        and os.path.exists(os.path.join(STORAGE_DIR, "index_store.json"))
    )

    if index_ready and not refresh:
        print('Loading index from storage...')
        storage_context = StorageContext.from_defaults(persist_dir=STORAGE_DIR)
        return load_index_from_storage(storage_context)

    #Index needs to be built and loaded
    index = build_index()
    
    return index


def main():
    console = Console()
    console.print("[bold cyan]Lahn River AI Avatar[/bold cyan]\n")

    refresh = input("Refresh Knowledge Base and System Prompt from Google Drive? (y/n): ").strip().lower() == "y"

    if refresh:
        fetch_system_prompt_from_gdoc()

    model_name = select_model()
    llm = get_llm(model_name)

    console.print("\U0001F4DA Preparing vector index...")
    index = build_or_load_index(llm, refresh=refresh)

    memory = ChatMemoryBuffer.from_defaults(token_limit=2000)
    chat_engine = index.as_chat_engine(chat_mode="context", memory=memory)

    console.print("✅ Ready to chat!\n")
    console.print(f"[bold green]Lahn River (Model: {model_name})[/bold green]")
    console.print("Type 'exit' to quit.\n")

    log_file = create_session_log()

    while True:
        user_input = input("[You]: ")
        if user_input.lower() in ["exit", "quit"]:
            break

        response = chat_engine.chat(user_input)
        console.print(f"[Lahn River]: {response.response}")
        log_file.write(f"You: {user_input}\nLahn River: {response.response}\n\n")

    log_file.close()
    console.print("\U0001F4C1 Chat session saved.")


if __name__ == "__main__":
    main()
