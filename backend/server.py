from flask import Flask, request, jsonify, send_file, make_response
from flask_cors import CORS
from werkzeug.utils import secure_filename
import os, io, asyncio
from datetime import datetime

# from llama_index.core import Settings
from llama_index.core.tools.query_engine import QueryEngineTool

from utils.avatar import get_llm, build_index, build_or_load_index, fetch_system_prompt_from_gdoc, search_text_index
from utils.utils import whisper_processor, whisper_model, transcribe_audio, azure_speech_response_func, LahnSensorsTool, format_history_as_string

import os

# === Initialize Flask ===
app = Flask(__name__)
CORS(app, supports_credentials=True)

UPLOAD_DIR = "data/uploaded_experiences"
os.makedirs(UPLOAD_DIR, exist_ok=True)

# === Load LLM once at startup ===
llm_choice = "gemma-3-27b-it" #"hrz-chat-small" #"gemma-3-27b-it" #"mistral-large-instruct" #"hrz-chat-small"

llm, system_prompt = get_llm('openai', llm_choice)

# print('LLM metadata model name: ', llm.metadata.model_name)

# agent=True
sensor_query_llm, _ = get_llm('gwdg', "hrz-chat-small", system_prompt= 'Provide an accurate response to the given query. Only perform calculations. Do not generate any plots or visualizations. Always include the following setup **before any resampling or time-based operations**: df[\'created_at\'] = pd.to_datetime(df[\'created_at\'])  df = df.set_index(\'created_at\') :')
query_llm, _ = get_llm('gwdg', "hrz-chat-small", system_prompt= 'Provide an accurate response to the given query:')

api_tool = QueryEngineTool.from_defaults(
        query_engine=LahnSensorsTool(sensor_query_llm),
        name=LahnSensorsTool.name,
        description=LahnSensorsTool.description,
    )


def prepare_query_engine(refresh=False):
    global query_llm
    if refresh==True:
        index = build_index()
    else:
        index = build_or_load_index()

    # query_llm = get_llm('gwdg', "mistral-large-instruct", system_prompt= 'Provide an accurate response to the given query:')

    # index_query_engine = index.as_query_engine(llm=query_llm,similarity_top_k=10, verbose=True)
    index_query_engine = search_text_index

    return index_query_engine, index


query_engine, index = prepare_query_engine()

debate_summary_llm, _= get_llm('gwdg', "mistral-large-instruct", system_prompt= '')
print('LLM initialized.')




@app.route("/api/refresh-prompt", methods=["POST"])
def refresh_prompt():
    global system_prompt, llm
    print('Refresh prompt request received.')
    fetch_system_prompt_from_gdoc()
    llm,  system_prompt = get_llm('openai', llm_choice)
    return 'Done.'



@app.route("/api/refresh-embeddings", methods=["POST"])
def refresh_embeddings():
    global query_engine, index
    print('Refresh embeddings request received.')
    query_engine, index = prepare_query_engine(refresh=True)
    return 'Done'



@app.route("/api/chat", methods=["POST"])
def chat():
    global llm, llm_choice, system_prompt
    print('Chat request received.')
    data = request.get_json()
    prompt = data.get("prompt", "")
    conversation = data.get("history", "")
    chat_history = []

    # if prompt == "__INIT__":
    #     prompt = "Hallo"
    #     # chat_history=[{'role':'user', 'content':prompt}]

    # else:
    chat_history = [
        {'role':"user" if m["sender"] == "user" else "assistant", 'content':m["text"]}
        for m in conversation
        ]

    # chat_history.insert(0, {'role':'user', 'content':'Hallo'})
    chat_history.insert(0, {'role':'system', 'content':system_prompt})

    # print('Chat history: ', chat_history)

    # print('After adding system and user prompts: ', chat_history)

    print('\nUser message:', prompt)

    results = ''

    # if 'get_relevant_Lahn_context' in response:
    print('Obtaining information for the LLM...')
    # response = response[response.find('user_query="')+12:]
    query = 'Provide context needed to address the most recent message in this conversation. Your job is not to predict what any party will say, but to provide information from the context, which is relevant for them to make their decision. That is where your job stops. : '+ format_history_as_string(conversation) + '\nUser: '+prompt #response[:response.find('")')]
    # print('Query: ', query)
    # context = query_engine.query(query).response
    context = query_engine(index, query)
    print('Context: ', context)
    context = '\n'.join(context)
    # results += '\nHere is the output of get_relevant_Lahn_context(): '+context

    chat_completion = llm.chat.completions.create(
          messages=chat_history+[{'role':'system', 'content':'Here is relevant information about the Lahn: '+context + ' . You can call analyze_sensor_data() if environmental data readings are relevant to the user\'s query.'}],
          model= llm_choice,
          top_p=0.8
      )

    response = chat_completion.choices[0].message.content

    print('Avatar response: ', response)


    if 'analyze_sensor_data' in response:
        print('Analyzing sensor data...')
        response = response[response.find('user_query="')+12:]
        query = response[:response.find('")')]
        print('Query: ', query)
        analysis = str(api_tool(query))
        print('Analysis: ', analysis)
        results += '\nHere is the output of analyze_sensor_data(): '+analysis +' Respond to the user accordingly. Do not provide any subjective Lahn-specific evaluation of this data, just focus on the quantitative result. And do not return a function call.'

    if len(results)>0:
        print('Passing analysis results to LLM: ', chat_history+[{'role':'system', 'content':results}])
        chat_completion_2 = llm.chat.completions.create(
              messages=chat_history+[{'role':'system', 'content':results}],
              model= llm_choice,
              top_p=0.8
          )

        response_2 = chat_completion_2.choices[0].message.content
        if 'analyze_sensor_data' in response_2:
            print('Duplicate function call for some reason')
            response_2 = analysis

        response_2 = response_2.replace('*','')

        print('Avatar response after getting sensor data:', response_2)

        return jsonify({"reply": response_2})

    # response = chat_completion.choices[0].message.content
    response = response.replace('*','')

    # print('Avatar response:', response)

    return jsonify({"reply": response})



@app.route("/api/debate-summary", methods=["POST"])
def debate_summary():
    print('Debate Summary request received.')
    data = request.get_json()
    conversation = data.get("history", "")
    topic = data.get("topic", "")
    summary = data.get("summary", "")

    formatted_history = format_history_as_string(conversation)

    prompt = f"""This is a debate between a human and an AI avatar for the Lahn river. Your job is to provide a summary outline in the format
            "Lahn:<Lahn's Central Perspective>\nPro:<Central Pro>\nCon:<Central Con of Lahn's perspective (deduced by you)>\n\nYou:<User's Central Perspective>\nPro:<Central Pro>\nCon:<Central Con of User's perspective (deduced by you)>", briefly outlining the Lahn's primary perspective, a pro and con of that perspective, the user's perspective
            and a pro and con of that as well. Keep all content very brief. You're summarizing, not re-iterating. You are provided with the most recent debate summary. If it already contains content, iterate on that content to reflect recent updates to the conversation.
            Topic being debated: {topic}

            Conversation:
            {formatted_history}

            Existing summary:
            {summary}

            Respond with an updated version of the summary in the described format. Make sure to preserve the specified formatting in the template "Lahn:\nPro:\nCon:\n\nYou:\nPro:\nCon:". No extra characters. The contents of your response should ba based purely on the given summary. 
            Summaries for 'Lahn' and 'User'should be based purely on what they said. If any party is yet to contribute to the conversation, leave their summary blank, as in the template."""

    response = debate_summary_llm.complete(prompt) #chat_engine.chat(prompt)
    # print('Summary model response: ', response)
    summary = str(response) #.choices[0].message.content

    # chat_history = [
    #     ChatMessage(role="user" if m["sender"] == "user" else "assistant", content=m["text"])
    #     for m in conversation
    #     ]

    # print('User message:', prompt)
    # response = chat_engine.chat(messages=chat_history)
    print('Summary:', summary)

    return jsonify({"summary": summary})



@app.route("/api/voice-chat", methods=["POST"])
def voice_chat():
    if "audio" not in request.files:
        return jsonify({"error": "No audio uploaded"}), 400

    audio_file = request.files["audio"]
    ext = audio_file.mimetype.split("/")[-1] 
    audio_path = 'data/temp.'+ext
    audio_file.save(audio_path)

    # audio_b64 = base64.b64encode(request.files["audio"].read()).decode()

    try:
        # run async function to get reply
        reply_text, reply_wav = asyncio.run(azure_speech_response_func(audio_path))
        # save reply to disk
        out_path = os.path.join("data", "reply.wav")
        with open(out_path, "wb") as f:
            f.write(reply_wav)
        return jsonify({
            "reply_text": reply_text,
            "reply_audio_url": "https://lahn-server.eastus.cloudapp.azure.com:5001/api/reply-audio"
        })
    except Exception as e:
        print("‚ùå Voice chat error:", e)
        return jsonify({"error": "Voice chat failed"}), 500
    finally:
        os.remove(audio_path)



@app.route("/api/reply-audio")
def reply_audio():
    # Serve the latest reply audio file
    audio_path = os.path.join("data", "reply.wav")
    if not os.path.exists(audio_path):
        return "", 404
    # Create response with CORS headers
    response = make_response(send_file(audio_path, mimetype="audio/wav"))
    response.headers["Access-Control-Allow-Origin"] = "*"  # or specify frontend origin
    response.headers["Access-Control-Allow-Headers"] = "Content-Type"
    return response



@app.route("/api/experience-upload", methods=["POST"])
def experience_upload():
    print("Experience upload received.")
    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S_%f")

    os.makedirs(UPLOAD_DIR+'/text', exist_ok=True)
    # Save the text message (if any)
    text = request.form.get("text", "")
    if text.strip():
        with open(os.path.join(UPLOAD_DIR+'/text', f"{timestamp}_message.txt"), "w", encoding="utf-8") as f:
            f.write(text.strip())

    # Save and transcribe the uploaded audio file
    if "audio" in request.files:
        audio_file = request.files["audio"]
        if audio_file and audio_file.filename:
            safe_name = secure_filename(audio_file.filename)
            file_ext = os.path.splitext(safe_name)[1]
            audio_path = os.path.join(UPLOAD_DIR, f"{timestamp}_audio{file_ext}")
            audio_file.save(audio_path)

            try:
                transcript = transcribe_audio(audio_path)
                with open(os.path.join(UPLOAD_DIR+'/text', f"{timestamp}_transcript.txt"), "w", encoding="utf-8") as f:
                    f.write(transcript.strip())
                print("üìù Transcription saved.")
            except Exception as e:
                print("‚ùå Failed to transcribe:", e)
                return jsonify({"status": "error", "message": "Audio saved, but transcription failed."}), 500

    return jsonify({"status": "success", "message": "Experience saved."})

if __name__ == "__main__":
    app.run(debug=True, use_reloader=False)
